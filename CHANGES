Sunday Morning, June 1st, 2014:
-------------------------------

    Add the connection request packet type to ClientServerPackets.h

    Added connection challenge, and challenge response packets.

    Each packet has a protocolId uint64_t, as well as a client guid uint64_t
    and a server guid uint64_t. This should provide enough safety that the
    packets belong to the same client, with same protocol and that connection
    can be established.

    Once connection is established, the 64 bit protocolId is passed along
    with each connection packet. This is overkill if travelling over a
    transport that already establishes connection, eg. UDPP2P on PSN,
    but for a raw UDP transport this is appropriate.

    I can repurpose this 64bits to support packet signing data
    and encryption payload, to avoid man in the middle attacks later.

    Added very rough code to generate 64bit guid. This needs to be revisited.

    Make sure the client generates a guid at the start of connect by address.

    This guid should be passed in to the server on connect request,
    and this guid is carried along from the client all the way to
    successful connect (to avoid mismatching client packets from
    a previous connect attempt...)

    Implemented state update for sending connection request.

    Implemented timeout if in sending connection request too long.

    Added unit test to verify sending connection request timeout.

    Actually start sending the connection request packets.

    Add a config for the send rate while in this state. 10 times per-second default.

    Now getting an error in sendto when actually sending the packets. Why?!

    Fixed. I thought it was defaulting to IPv6 but actually was defaulting to 4.

    Write the client side code to receive incoming packets.

    Knowing that only the client may receive packets on the interface
    leads me to make the client responsible for updating the interface
    as well, and updating the resolver.

    Added code to process packets, if the packet is connection challenge
    and the client is in correct state, and all data matches, client will
    transition to the sending challenge response state.

    This is probably as far as I can go on the client until I implement server side.


Saturday, May 31st, 2014:
-------------------------

    First, start by defining the client and it's state machine, behavior etc.

    Client will have a single connection, a network interface it uses to
    send packets across.

    Client/server will have to have their own packet enum, packet factory
    implementation, as they need a bunch of their own packets for comms.

    Add resolver and interface to client and server configs.

    Add ClientServerPackets.h

    Added tests/TestClientServer.cpp (no point testing in isolation!)

    Sketched out the set of tests I want.

    Designed the client state and the set of errors.

    Started work on basic tests, eg. initial state.

    Added lots of nice accessors, IsConnecting, HasError, IsConnected
    IsDisconnected and so on. There functions are shorthand, but it makes
    the code quite a bit clearer vs. checking state/error enums all the time.

    Implemented internal functions for connect/disconnect, and error setter,
    clear and so on.

    If hostname is an address string, use that and connect by address instead.

    Implemented hostname lookup via resolver. Added per-state data so the
    state machine remains really clean. Added fn. to clear all state data,
    and made sure it gets called on disconnect.

    Should now have code that actually performs the resolve, waits for
    succeed or fail result, as well as having timeout specified in config
    at which point it gives up (important for every state!)

    Add a unit test to verify the timeout works, trick would be to not
    update the resolver. No sleep required either.

    Add test to verify the hostname resolve works, eg. use "localhost"

    Added code to transition into "sending connection request" state
    inside connect by address fn.

    Enough work for today. Time to take a break.


Friday Morning, May 30th, 2014:
-------------------------------

    Implement ack test. Randomly drop packets and verify that the invariant holds
    that the acked packets must have been received, and that not received packets
    must not be acked.

    Note that it's possible for acks to be lost, so some received packets will not 
    be acked.

    Need a way to determine when a sequence has been acked.

    Probably the simplest way is to implement a dummy channel that hooks in
    to the connection and has "ProcessAck" called on it.

    Added test, works fine.

    Make sure all tests call srand( time( NULL ) ) at the beginning,
    so they have actual random numbers, vs. the same sequence every
    time they are run.

    Add a reliable message channel test that mixes messages, large and small blocks.


Thursday Night, May 29th, 2014:
-------------------------------

    Add latency and out of order packets to reliable message channel tests.

    Found a breakage in the large block test!

    Wow. Fixed it to throw an assert, and it seems that it is now stalled out.

    Soak test hangs too if you leave it long enough. Seems to hang right after
    a long block is received, going back to bitpacked messages / small blocks.

    I have a 100% quick repro with the large block test, so debugging there first.

    First see what it is hanging up on, log the packet discards... Nope.

    It seems to be another ack related issue.

    My theory is that the sender has missed some acks, so although the receiver
    has definitely received the packet (and has advanced on), the sender does not
    know that he is finished, and can stop sending this block.

    I think the fix is for the receiver to only discard the packet if it is a large
    block id that is more recent than the current message id. Older block ids need
    to get acks back to resolve this situation.

    Yep. That did it!


Thursday Morning, May 29th, 2014:
---------------------------------

    It would be nice if the soak test would cycle in-out of really bad conditions

    eg. cycle between 0 packet loss and 100% packet loss.

    Cycle between no latency and 10 seconds latency.

    Cycle between no jitter and 1 second jitter.

    The goal of the stress test is to test the worst possible conditions.

    This means that the test actually has to exercise these worst case
    conditions, eg. 10 seconds with no packets getting through, does it
    recover?

    It would be nice if the simulator itself had the ability to cycle between
    different states with probabilities, eg. create a bunch of "state" structs
    with settings for latency, jitter, packet loss -- and assign a probability
    to each (which gets normalized), then pick a new state randomly with 1 in n
    chance (n in config) every update.

    It's great to put this in the simulator, vs. having it in the SoakTest,
    because this same logic can be used anywhere the simulator is used.

    First step, split out the simulator into NetworkSimulatorConfig (create time)
    and NetworkSimulatorState (dynamic).

    Added "AddState" function. This way you can add states to the sim so that
    it oscillates between them with equal probability. I don't really think
    the weighting by priority is worth the effort. This is good enough.

    Verified that the state is adjusting every n frames.

    Soak test works fine.


Wednesday Morning, May 28th 2014
--------------------------------

    Add "NetworkSimulator.h" to delay packets and deliver them
    after a fixed amount of delay, with +/- jitter in seconds.

    Made network simulator implement the network interface.

    Seems to make the most sense.

    Add latency and out of order packets to soak test.

    Seems mostly fine, but when I increased the amount of packet
    loss and latency, it seems I have broken it.

    Interestingly, just setting latency to 1 second is enough to break it.

    Also, it still breaks when I only send messages. Large/small blocks not required.

    It breaks *precisely* at 512, which is the receive queue size.

    What is going wrong?

    It seems that all messages are being discarded on receive because they are "old"

    My guess is that somehow the receiver is running ahead, eg. has the sliding
    window advanced forward, further than will allow the non-received messages
    to ever be received.

    It seems like "ProcessAck" is not getting called.

    Why is this?

    Acks are also super weird, eg. for a long time:

        65535 - 0

    And then it ends with persistent:

        n - 55555555

    Which is really strange as well. Why 55555555?

    In binary this is 010101010101...

    Why are acks coming through for odd packets?

    Some seriously fucked up stuff here.

    Some clues here:

    ack patterns are consistently 0101010 etc.

    This persists even if:

    a) bsd sockets are removed
    b) simulator is removed

    This is very strange. Is something wrong with acks or is send sequence getting
    bumped by two somehow? eg. ++, ++?

    Yes. It looks like sequences are going up by two somewhere.

    This *may* be a separate issue to why the event queue stalls out at 1 sec latency.

    Yes, the sliding window change I made was adjusting the sequence ahead at twice
    the rate, eg. 1, 3, 5, 7 etc...

    This was also causing the broken acks, eg. the sent packet data did not match
    the actual packet sequence, thus lookup for acks was broken, acking the wrong
    messages in the send queue and causing the messages to stall out.

    Fixed!

    Reintroduced mix of bitpacked messages, small blocks and large blocks into
    soak test. Everything is working fine again.


Monday May 25th, 2014 (Memorial Day)
------------------------------------

    Add some rare large blocks to the soak test. 

    Working fine.

    Bumped up soak test so it's really going to be pushing the limits of CPU,
    eg. larger packets, large blocks, increased message size etc.

    During soak test it would be nice to see received x/y fragments of large block.
    This way there is continuous logging and it is a nice steady stream, vs. the current
    large hitch on each block which is occuring.

    Added GetSendLargeBlockStatus and GetReceiveLargeBlockStatus to return 
    structs providing information about current send/receive large block.

    Used these to display x/y fragments while receiving a large block.

    Extend soak test to actually validate contents of blocks properly,
    eg. make each byte a modulus of byte index in block.

    Make the small and large block data more distinct, eg. each byte
    a unique modulus of byte index and sequence, and verify each byte
    is correct on receive.

    Unit test should have some bit pattern for message bits,
    so they can be verified on the way back.

    Solved by adding a magic 32bit value after the bits.


Sunday Evening May 24th, 2014:
------------------------------

    Things to do to get the large block codepath working:

        1. Implement code to include a fragment in packet data          <-- DONE

        2. Implement code to fill the sent packet entry                 <-- DONE
           for fragments, so they can be acked. Need both
           message id for block, and fragment id.

        3. Implement code to process ack, lookup block by id            <-- DONE
           id and fragment, and mark that fragment as acked,
           and once all fragments are acked, go to next msg
           in send queue.

        4. Implement code to serialize read/write fragment data.        <-- DONE
           It's OK to copy the fragment data from the packet to/from
           the actual block itself, this is no problem given small
           fragment sizes, and it makes the code implementation
           much easier (actual block remains private to the channel)

    Hey, I have to store the actual block size inside the serialize data
    otherwise the receiver has no idea of the actual size. I think just
    sent a 32bit block size with each fragment, that way any fragment
    arriving first will prime it up. Done.

        5. Implement code to take that serialize fragment data
           and copy it in to the block itself on receive,
           track received vs. not received fragments, and
           queue the actual block up in receive queue once
           all fragments have been received.

    On receive side of block I need:

        - active true/false whether we are receiving a block right now
        - blockId that is currently being received
        - size of the block that is being received
        - number of fragments in block that is being received
        - number of received fragments in block so far

    When a fragment comes in for a block, first check if we are already
    receiving that block.

    If not receiving, check that we have received all messages ids 
    prior to the block message id, if not, then discard packet
    and bump a counter. This should "never happen".

    If already receiving, verify the fragment id is in bounds, 
    and then check that the block size still matches.

    If we are receiving a block, discard any packet that arrives
    containing a different block, or non-block messages. This should
    be an exception and raise a counter because it should "never happen".

    Had to adjust the sliding window so that "GetSequence" returns
    the next expected sequence to insert, vs. the current one -- which
    was undefined and not giving me the result I wanted for my "is this
    large block id the expected next mesasge to be received" check.

    Had to adjust various tests, as well as GenerateAckBits to work
    with this new scheme. But it makes more sense and the code is
    clearer post this adjustment.

    Seems there is some data corruption on receive. A byte at a weird
    index (1985?) is zero in the second received block. Why is this?!

    Gotta debug why the block is getting corrupted.

    Seems like it is, at least for the first block being sent, off by 64

    eg. 1024 - 960 = 64

    My guess is an off by one on the last fragment?

    Passes now for first block, but second block which is the first
    one that has a non-even multiple of fragments, has a problem with
    the last 64 bytes

    eg. 2049 - 1985 = 64

    My guess is that I'm off by one in the case of the block with remainder?

    Yep. Off by one. Was not dividing as float inside ceil.

    Now I'm getting an assert in the large block test, under packet loss:

    Assertion failed: (data.blockId == expectedBlockId), function ProcessData, file ../include/ReliableMessageChannel.h, line 589.

    Is this a valid assertion? It seems to indicate that a block is coming in
    but with an unexpected message id. But can this possibly happen, legitimately?

    It would seem it could happen if a block goes incorrectly to start sending
    the next block, before the current block is fully acked. Is this happening?

    It seems that under high packet loss the sender can still be thinking
    that he is on block 12, when the receiver is ready for block 13.

    I'm guessing this is just a delayed reaction, eg. some acks that don't
    get through. The sender recovers and works just fine, but exception throwing
    in packet processing, it's not an exceptional thing -- can totally happen.

    These aren't conditions that are assert worthy. They can happen in the real world.

    Are there other conditions I thought which were imposible, but can actually
    happen under high packet loss? Probably!


Sunday May 25th, 2014:
----------------------

    User should be able to specify large block fragment size in config,
    and maximum block size (so we can derive the max # of fragments)

    I think having up to 16 bits fragment index is plenty. If you want
    large blocks, you'd naturally want larger packets and larger fragments,
    otherwise it's just a silly exercise to think to try to send a gig
    file through 32 bytes at a time =p

    Need to allocate a structure up to the maximum fragment size, one
    on send and other on receive side, for per-fragment data.

    The structure should be extremely well packed to efficiently support
    large blocks, and be cache coherent.

    The actual block data itself should be dynamically allocated
    and passed around via shared_ptr. Blocks are large. No copying!

    The per-sent packet sliding window needs a new entry showing if
    this packet had a block fragment, or if it contains messages.

    Send queue entry needs a "largeBlock" bit for easy processing
    of the message /small block mode, it needs to go only up to the
    next large block.

    Added code to break on nullptr send queue entry, because that should mean
    STOP, there is no entry for this message id. Need to verify with soak test.

    Yep. Works fine.

    Added split for "large block" vs. "message and small block" mode inside
    get data. Need to implement the large block mode codepath. For the moment
    it just returns nullptr data (stalls out)


Saturday May 24th, 2014:
------------------------

    Shrink the message size in soak test now that we have bitpacker

    Add overflow checking for bitpacker read and write.

    Add exception on overflow for serialize read. This is the important one,
    because this data can be coming in from an untrusted source.

    When writing a packet, it is expected that programmer error will be
    the only source of writing past the end of the buffer, thus the checking
    is performed via asserts. No need for exceptions in this case.

    Implement code to measure messages in send queue.

    Ensure that this measurement is done only once as it is expensive.

    Cache the message overhead # of bits in ctor so it doesn't have to be recalculated.

    Shrink the size of the packet in soak test so we can't fit all messages in the packet.

    Also jack up the max # of messages per-packet to 64, so small messages can get packed in.

    Implement code to count the # of bits for messages when searching for messages to include
    in the send data.

    Make sure there is code that gives up after a small # of bits remaining, so the code
    doesn't walk through the remaining send queue entries unnecessarily when no message
    can possibly/reasonably fit.

    Make sure we never go over max packet size in soak test

    Make sure the reliable message test passes


Monday May 19th, 2014:
----------------------

    Extend message channel to support *small* block messages

    This can be implemented just by checking the block size, and implementing a serialize
    that just serializes the bytes to the stream via the bitpacker.

    Extend MessageChannel interface to support sending a block (vector ...) directly.

    Assert if message is too large to include (add small block # of bytes in config)

    Extend soak test to include non-block messages with random # of bits

    Optimize block serialization to serialize by words (4 bytes at a time)
    with tail serialized one byte at a time. Less calls to serialize,
    and more efficient journalling (although, journalling should be,
    if implemented, coded to be aware of the block, eg. serialized block
    n bytes long, should be the journal entry...)

    Write a generic method, serialize_block to provide block serialization
    outside of the block message. This is generally useful functionality.

    Extend the soak test to include block messages of random # of bytes

    Implement the bitpacker.

    Keep bitpacker separate from the stream class for maximum flexbility.

    Bitpacker should work with raw pointers, not vectors.

    Bitpacker should not use or throw exceptions. Leave this to stream layer.

    Split into separate reader and writers. I don't want unified read/write.

    Implemented bit writer.

    Added unit test to make sure # of bits are written, bits available correct etc.

    Using htonl for endian conversion. Later on my want to use my own routine,
    otherwise this will require me to include headers just to get htonl.

    Added some code that will let me detect endian at runtime, without
    needing to pass in #define BIG_ENDIAN or LITTLE_ENDIAN.

    Is there a better way to do this? eg. at compile time vs. runtime?

    Apparently there is not.

    Implement bit reader.

    Fixed bug on write that set all bits to 1. Was incorrectly trying
    to clear high bits on write bits, ended up setting all bits to 1.

    Got it apparently working but I'm getting some puzzling logs.

    The read and write words are not matching.

    This is puzzling. How can these logs be wrong, when the values
    being read back from the bit reader are correct?!

    Dumbass. It was just endian flip.

    Hook bitpacker up to the net stream.

    Make sure stream tests pass.

    Make sure reliable message channel test passes.

    Make sure soak test still works.

    Implement serialize_bits( stream, int, num_bits ) -- this is very useful

    Converted serialize_bits and serialize_int to macros so they work with 
    bitfields. Can't pass a bitfield in by reference. Macros work.

    Pass over existing serialize_int and switch over to serialize bits where appropriate

    Add serialize_bool

    Passed over code using serialize_int and switched to bool where in range [0,1]

    Added optimization to ack_bits. If it is perfect (0xFFFFFFFF), serialize one bit.

    There is a problem with TestReliableMessageChannel, seems that some data is messed up (message id?)

    Fixed. Write stream was missing a flush.

    BSDSocketsInterface.h needs conversion (it uses serialization internally)

    Verified BSD sockets test passes

    Convert the SoakTest.cpp over

    Verify it still runs.

    Time for bed!


Sunday May 18th, 2014:
----------------------

    Extend soak test to send and receive messages

    Remove logs except send and receive message n. 

    Make sure all asserts were being checked. #ifndef NDEBUG

    Fixed bug with adding messages to packet. Message id had modulus
    applied so after sliding window size, no messages from send queue
    where being put in packet data.

    Some work to ensure that send -> receive messages is the minimal
    number of frames, ideally messages should be received at the end
    of the same frame they were sent, for loopback.

    Works fine without packet loss, but with packet loss it stalls out

    If I disable the last send time check for resending a message,
    it works again. It seems there is something wrong with resend
    logic?

    There was an off by one when the send queue was full,
    such that message 0 was never being resent, once the send
    queue filled up.

    Also, in the case where the receive queue is smaller than
    the send queue (common), the send side was sending early
    events to the receiver, which is counterproductive.

    Fixed by only sending events up to the size of the receive
    queue from the oldest message in send queue, nothing more
    recent than this is ever sent.

    Structure source code, eg. include / src / tests

    Update C++11 build system to look up for ../includes


Saturday May 17th, 2014:
------------------------

    Sketched out soak test.

    For some reason packets aren't being received from the interface

    Fixed. I was not passing in a resolver and the update was returning
    early if resolver was null (not running send packets)

    Split out resolver update into "UpdateResolver" so the other 
    update part is not accidentally being skipped with null resolver.

    When connection packet is deserialized it asserts because it doesn't
    get registered with its connection interface.

    I think I should return the connection packet to require a connection
    interface being passed in, and a custom type vs. hardcoded to some
    value, this way the factory is responsible for its creation!

    Solution is probably to have a custom factory that does this registration.

    Rename "factory" in configs to "packetFactory" or "messageFactory"
    to make it clear. There will not always be one factory in context.


Thursday May 15th, 2014:
------------------------

    Fixed up connection packet so it conforms to standard factory interface.

    Fixed TestConnection.cpp so it works again.


Tuesday May 13th, 2014:
-----------------------

    Restructure code into various headers so I can have multiple programs.

        Common.h
        Stream.h
        Network.h
        Connection.h
        MessageChannel.h

    Split up unit tests into separate cpp per-aspect, eg. TestMessageChannel.cpp

    Split up the code so that each section has minimum dependencies.

    Problem. I would really like packet to be independent of network.

    Solution: Move address out of network.h

    Moved Packet.h out of Network.h

    Moved Channel.h out of Connection.h

    Moved Message.h out of MessageChannel.h

    Move various X::Configs out into their own XConfig structs. More flexible.

    Create test harnesses for each module:

    eg: 

        TestCommon.cpp
        TestStream.cpp
        TestNetwork.cpp
        TestConnection.cpp

    Fixed up test factory to be less verbose. Cut down packet types.

    Fix TestNetworkInterface.cpp

    Separate DNS resolver implementation from resolver base class

    Move resolver tests into TestDNSResolver.cpp

    Separate BSD socket implementation from network interface base class.

    Keep 'test_network_interface' into TestNetworkInterface.cpp

    Move other tests into TestBSDSocketsInterface.cpp


Monday May 12th, 2014:
----------------------

    Implement the code that serializes the message types, and uses the 
    factory on the reading side to create the message by type, and then
    serializes the message.

    Does the message really need to include the id? Yep. It does.

    Was clobbering the message id to 0 inside serialize write.

    Seems to be something wrong with serialization or test harness.

    Message coming in with "sequence" (inside TestMessage) that is
    the last message sent, but it is in receive queue as 0 (and message id 0). 
    Almost like the message ptr is being reused, or flipped incorrectly at 
    some point. 

    What is going on?!

    Was just an inversion of the serialize for messageId. Was writing 0
    instead of the correct message id.

    Now the test harness works fine!


Sunday May 11th, 2014:
----------------------

    Added receive queue data and added receive queue to message channel

    There is now a receive message id starting at zero. Will try to receive
    this message id from receive queue, will block other messages until
    it is available.

    Added code to receive messages from queue.

    When a message channel data comes in add the messages in it to the receive queue.

    If any message in the packet data is newer than the current recieve message id,
    discard the packet before doing any work (otherwise, messages will be acked
    potentially, and never resent).

    If a message is older than the last receive message id, ignore it.

    Implement the counters so that the unit test can detect correct operation
    (eg. messages sent/received/read/written/discarded-too-new)

    Implement the code to serialize the message channel data on write, and receive.

    Need some way to pass the max messages per-packet, and other config data
    in to the message channel on serialize. How do to this? Allow the channel
    to pass it's data in via some opaque ptr in on serialize?

    Would be nice to have a *general* mechanism to do this, as you are going 
    to always want to access the config data while doing work for that channel.

    Settled on simply passing a pointer to the channel config in to the 
    channel data on creation. This makes the most sense in my opinion.


Monday May 5th, 2014:
---------------------

    Add code to gather n messages to include in packet serialize write

    Add code to ignore messages that have already been sent within "send rate" of current time.

    Don't send messages that are > send queue size / 2 ahead of oldest message in send queue.

    Ignore message packing. Assume that max # of messages in packet will not exceed max packet size.

    Added id to message. Makes a lot of other stuff easier vs. maintaining separate data structure
    to have the id and message ptr everywhere.

    Set the message id when it is queued for send.

    Implement the message channel data class.

    Add vector of messages to message channel data.

    Verify messages are correctly setup via prints.

    Need an "add fast" option for struct like entries for which copying is bad,

    eg. sent packet entry has a vector. I don't want to copy that!

    Added "InsertFast" method that just sets valid 1, sets sequence then returns entry pointer to fill.

    Add sliding window for packets "m_sentPackets" with a vector of size max messages per packet (uint16_t ids)

    Add code to process acks and walk the m_sentPackets, if not already acked, mark that packet acked, 
    and then walk over all messages in send queue and remove them from the send queue.

    Add logs to verify this is working properly on the send side.


Sunday May 4th, 2014:
---------------------

    Sketched out new "MessageChannel" class and supporting structures.

    Added new "Message" base with "BlockMessage" being hard-coded to type 0 (saves extra virtual for "IsBlock")

    Added new "TestMessage" with sequence #

    This should provide all I need to test the message channel is working.

    Added simple config for message channel.

    Add the message factory to the config.

    Sketched out test harness for message channel.

    Now implement "SendMessage", "ReceiveMessage" interface on channel and have failing test.

    Send queue needs to be adjusted to a sliding window.

    Added new function "HasSlotAvailable" to send queue to detect if we have overflowed it.


Saturday April 12th, 2014:
--------------------------

    Hook up the "resend rate", eg. don't send the block down the channel continuously
    resend it at some fixed rate, eg. 10 times per-second.

    This requires current id for block and a time since last send of that id.
    Set this block id initially to 0xFFFF and if it is different to current
    block, update block id and immediately send block, set time to 0.0

    Added simple network timebase. Used doubles to keep it simple.

    There seem to be some problems with ack processing for reliable blocks.

    Debug it with logs. Is the ack processing to blame, or the reliable block channel?

    Error appears to be in the ack system. It's off by one somewhere, eg. acking packet 10
    but packet 10 was not received (packet 10 was).

    Yes, there was an error in the generate ack bits function.


Thursday April 4th, 2014:
-------------------------

    Make sure an exception thrown while processing channel data discards the packet
    (ensures that packet does not get marked as acked).

    Added counter for packet process fail.

    Add a unit test to verify this is the case, eg. create a test channel that has
    this behavior, and verify that no packets are acked while sent through with
    the exception being thrown.


Sunday March 30th, 2014:
------------------------

    Sketched out new reliable channel, channel data classes.

    Reliable channel will initially only send only one block at a time.

    Initially just resend the most recent block until it is acked.

    Added sliding windor for sent packets. This provides constant time
    lookup from the acked sequence to the block id.

    Added code to pop the block from the send queue if the acked
    block id matches the front block id in the send queue.

    On the receive side, if the block is not the id we are expecting
    then the process packet should throw an exception when processing

    This exception should discard that packet so it does not get acked.

    If the recieve side has the block id it expects, then it should
    simply queue up that block and id into the receive queue.

    Implement unit test with simulated packet loss.

    Verify blocks get through reliably, and in-order, eg. block contents
    should be i, where i is the i'th packet sent.

    Extend unit test to verify generate ack bits is working correctly.

    Found a bug in sliding window. Was not clearing entries on reset properly.


Tuesday March 25th, 2014:
-------------------------

    Try to implement basic dumb channel.

    Serialize write is working. Everything up to including channel data
    in the packet, and then serializing this channel data with skip bits
    works fine.

    Added interface that lets connection packet:

        1. Get the number of channels
        2. Create data for a channel by channel #

    Made this a shared_ptr owned by the connection, that way it can be shared
    with the packets, even if the connection object itself is not a shared ptr.

    Serialize read is working, reading blocks.

    Next, process the data on serialize read.

    Verify send -> serialize write -> serialize read -> receive channel block.

    Clean up logs and adjust unit test to iterate until 10 blocks received.


Monday March 24th, 2014:
------------------------

    Added channel interface.

    Seeing as we don't actually serialize the data *at all* inside the connection
    we cannot directly serialize channel data from inside connection processing.

    Added new object "ChannelData" derives from "Object" so it has serialize.

    Then added new interface to channel to drive this data:

        virtual shared_ptr<ChannelData> CreateData() = 0;

        virtual shared_ptr<ChannelData> GetDataForPacket( uint16_t sequence ) = 0;

        virtual void ProcessDataFromPacket( shared_ptr<ChannelData> data ) = 0;

        virtual void ProcessAck( uint16_t ack ) = 0;

    This should provide enough information to implement any channel strategy.

        1. You can get a data object to include in a packet with sequence
        2. If there is no data available right now, return nullptr. (drives skip bits)
        3. On read, create data then serialize in -- once serialize has completed
           call process data.
        4. Process ack should provide enough to implement reliability

    For the channel to abort packet send or packet read, it should throw an exception.

    This exception will abort the entire packet read and discard the packet.


Sunday March 23rd, 2014:
------------------------

    Work out how the user adds channels and works with them?

    User creates the channels and adds them to the connection via shared ptr.

    User can then directly work with the channel specific interface,
    while the connection works with the base channel interface to 
    serialize read/write packets.

    This keeps the connection functionality separate from the channel
    specific functionality.

    I could have lots of different channel types, eg. JSON channel,
    CaptainProtoChannel, ProtocolBuffersChannel, RawChannel, EventChannel
    etc.

    The key is that the connection works with data blocks, and 
    aggregates them into a single packet with reliability.

    This is different to, and less efficient that past techniques
    I've done where the serialization is done entirely in-place.

    But it makes more sense and is easier to integrate different
    channel types into it, at the cost of only a small amount
    of inefficiency.

    Implement a simple channel serializer that just sends blocks of data
    eg. UnrelaibleBlockChannel -- stupid serialize sends bytes per-block, then
    block data. Only one block per-packet.

    This is a tracer bullet... what does the connection need from each
    channel in order to work properly?


Sunday March 16th, 2014:
------------------------

    Add sent packets to the sliding window

    Add received packets to the sliding window

    Extend sliding window to return false if packet is too old
    to add to sliding window, eg. discard packet

    Properly implement "sequence_less_than" and "sequence_greater_than" functions, 
    and test that they correctly handle 16 bit wrap around.

    Fixed bug in sliding window. Uninitialized data in ctor.

    Detect if received packet is too old to add to sliding window. Discard packet.

    Implement "GenerateAckBits" function.

    Unit test the generate ack bits function (it should basically take a sliding window and return ack, ack_bits)

    Yep. It was slightly off with or then shift. needed to be shift then or.

    Implement the "ProcessAcks" function to take ack + ack_bits and check for sent packets matching,
    call "PacketAcked" if it is the first time a sent packet with that sequence was acked.

    Add counters and include basics stats such as number of packets writter, read, acked, discarded.

    Extend unit test to verify counters match expected results, eg. acks especially.


Saturday March 15th, 2014:
--------------------------

    Should there be nodes? Yes. But strictly client/server. No support for P2P.

    This means there is just the server, with data per-client, and the client's
    view of the server data that is synchronized to it.

    So calling each connection a "client" instead of a node is probably correct.

    But a node is more generic, and it makes sense for a server to have n nodes,
    one per-client, but on the client to just have one node.

    Alternatively, it could be called "connection"?

    Yes. Going with connection. I can then later on create a server class
    with one connection per-client, and a client class with a single connection
    to the server. SORTED.

    Sketched out connection packet with sequence, ack, ack_bits.

    Sketched out functions and data structures required to implement reliability.

    Implement sliding window class.

    Maybe consider moving the sequence numbers, eg. most recent sent packet, most recent received *into* the sliding window.

    Maybe add a bool into sliding window, eg. have you ever had an entry inserted yet? (eg. for initial sliding window set sequence)

    Yes. Everything becomes simple if the sliding window class is well designed like this...

    Sliding window is basic. But it seems additional logic is required, like how to 
    detect if a received packet is in the window. Discarding of old packets etc.

    This logic I think is specific to the send and receive sliding windows.

    I will keep it out of the sliding window class.

    Unit test the sliding window class.


Friday March 14th, 2014:
------------------------

    Get the network interface working across IPv6 too ("::1")

    Now adjust so you can create the network interface in IPv4 mode or IPv6 mode.

    Currently using AF_INET to mean IPv4, AF_INET6 to mean IPv6 only and
    AF_UNSPEC to mean dual stack. This may not be the best.

    Do I care about dual stack at this point? No I don't. Cut dual stack support.

    Add query fn to get maximum packet type from factory. Necessary for serialization.


Wednesday March 12th, 2014:
---------------------------

    Add test to verify send and receive packet by address.

    Create non-blocking socket inside network interface.    

    Clean up socket in network interface dtor.

    Fixed address initialize from addrinfo to include port.

    Hack up the sendto to work with IPv6 address types.

    Added comparison operators to packets and learned about static_pointer_cast.

    Added code to verify the packets received on the other side verify the 
    packets that were sent, before serialize read and write.

    Hacked up a really nasty serialize write buffer for stream. I'll fix this up later.

    Implemented internal packet send code that calls through to sendto with
    serialize write buffer contents.

    Need a config to pass into the network interface, we'll need to specify
    the maximum packet size in there, otherwise we're going to not know
    what size buffer to use for recvfrom.

    Need to pass factory into network interface. It needs it so it knows
    what the maximum packet type is (for serialize).

    Added network interface config. config specifies max packet size,
    packet factory, dns resolver (optional) and UDP port to bind to.

    Call recvfrom and get packet data. 

    Some problem where recvfrom is always returning -1. Possible that my address is wrong
    and it is always WSAWOULDBLOCK? Need to dig in and get socket error in this case.

    Address int32 was double converted using htonl. Fixed.

    Serialize read this buffer into a stream and then determine the packet type.

    Create an object of the correct type. If packet for type cannot be created, 
    increment a counter and discard the packet.

    Serialize the packet. If the packet serialize fails discard the packet
    and increment serialize read failure counter.

    If the packet serializes properly, add the packet to the receive queue


Sunday March 9th, 2014:
-----------------------

    Implement lookup from hostname to address inside network interface via resolver

    Added counters to network interface to aid in unit tests (eg. num packets sent, received, discarded)

    Add test to verify hostname lookup + resolve send queue + send on resolve works

    Add test to verify hostname failure discards packets

    Carry the port # from send to address -> internal send queue (resolve). Verify with logs.


Tuesday March 5th, 2014:
------------------------

    Need a way to resolve DNS into addresses.

    Add a new interface "Resolver". 

    Derive from this and create a new "DNSResolver"

    This will keep it generic, for example I could create redis
    based server resolver for giving game instances server names,
    or a master server list etc.

    By having a resolver interface I won't care how the name -> address
    resolution occurs, just that it *does* occur.

    Added way to initialize address from addrinfo ptr (from resolve DNS...)

    Cache data on lookup so it can be queried.

    If an address has already been looked up, don't fire off a new async for it.
    (but must you call the callback?! not sure... probably.)

    There should have the option to clear the cache.

    There should be a way to look up an address without calling lambda.

    Optimize DNS resolver so that in-progress queries are in a separate hash,
    so we don't go O(n) in the udptae after lots of queries have been done.

    Verify that multiple overlapping resolve calls work, and all the lambdas are called on completion

    Verify resolve failure works as well, and cached fail result.


Sunday March 2nd, 2014:
-----------------------

    Basic serialization, serialize_int etc.

    Base object for serialization with virtual Serialize( Stream & stream ) method.

    Back packet class with serialize, address string (eg. sent from), and type int.

    Factory class to create packets from type.

    If you try to create an object of a type that is not registered, throw a runtime_error.

    Explore "getaddrinfo" for IPv6 aware hostname -> IP address etc.

    Implement basic address class capable of handling IPv6 and IPv6

    Implement function to convert IPv4 string to address int + port

    Verify IPv4 UDP broadcast address is converted properly: 0xffffffff

    Implement function to convert IPv4 address + port back to string
    (Omit the port if it is zero).

    Added code to initialize IPv6 with 16bit pairs, including htons
    (decided on network byte order for internal address data...)

    Handle port number in IPv6 output string, eg. "[v6addr]:portnum"

    Implement IPv6 parse from string

    Implement IPv6 parse from string with port, eg. "[addr6]:portnum"
